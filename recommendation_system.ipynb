{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Github Projects\\Project\\Recommendation_System\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(url: str):\n",
    "\n",
    "    folder = \"Dataset\"\n",
    "    file_path = os.path.join(folder, \"data.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Data already exists at : {file_path}\")\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            response = requests.get(url = url)\n",
    "            print(\"Successfully downloaded the dataset from url.\")\n",
    "\n",
    "            os.makedirs(folder, exist_ok = True)\n",
    "            print(\"Created a new Data folder to stiore the dataset.\")\n",
    "\n",
    "            with open(file_path, \"wb\") as file:\n",
    "                file.write(response.content)\n",
    "\n",
    "            print(f\"Successfully downloaded the data in the location : {file_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded the dataset from url.\n",
      "Created a new Data folder to stiore the dataset.\n",
      "Successfully downloaded the data in the location : Dataset\\data.csv\n"
     ]
    }
   ],
   "source": [
    "url = \"https://github.com/611noorsaeed/All-Recommendations/raw/refs/heads/main/Articles.csv\"\n",
    "file_path = download_dataset(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(file_path, encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Date</th>\n",
       "      <th>Heading</th>\n",
       "      <th>NewsType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KARACHI: The Sindh government has decided to b...</td>\n",
       "      <td>1/1/2015</td>\n",
       "      <td>sindh govt decides to cut public transport far...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HONG KONG: Asian markets started 2015 on an up...</td>\n",
       "      <td>1/2/2015</td>\n",
       "      <td>asia stocks up in new year trad</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HONG KONG:  Hong Kong shares opened 0.66 perce...</td>\n",
       "      <td>1/5/2015</td>\n",
       "      <td>hong kong stocks open 0.66 percent lower</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HONG KONG: Asian markets tumbled Tuesday follo...</td>\n",
       "      <td>1/6/2015</td>\n",
       "      <td>asian stocks sink euro near nine year</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEW YORK: US oil prices Monday slipped below $...</td>\n",
       "      <td>1/6/2015</td>\n",
       "      <td>us oil prices slip below 50 a barr</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article      Date  \\\n",
       "0  KARACHI: The Sindh government has decided to b...  1/1/2015   \n",
       "1  HONG KONG: Asian markets started 2015 on an up...  1/2/2015   \n",
       "2  HONG KONG:  Hong Kong shares opened 0.66 perce...  1/5/2015   \n",
       "3  HONG KONG: Asian markets tumbled Tuesday follo...  1/6/2015   \n",
       "4  NEW YORK: US oil prices Monday slipped below $...  1/6/2015   \n",
       "\n",
       "                                             Heading  NewsType  \n",
       "0  sindh govt decides to cut public transport far...  business  \n",
       "1                    asia stocks up in new year trad  business  \n",
       "2           hong kong stocks open 0.66 percent lower  business  \n",
       "3             asian stocks sink euro near nine year   business  \n",
       "4                 us oil prices slip below 50 a barr  business  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"English\"))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Tokenize\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove stopwords and apply stemming\n",
    "    preprocessed_text = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "\n",
    "    return \" \".join(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machin abil machin learn'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(\"Machine is the ability of machine how to learn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"cleaned_heading\"] = dataset[\"Heading\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Sentence Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name: str):\n",
    "    try:\n",
    "        model = SentenceTransformer(model_name)\n",
    "        print(\"Model downloaded successfully.\")\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Github Projects\\Project\\Recommendation_System\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rahul\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model = get_model(\n",
    "    model_name = \"all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Embeddings using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embds(model):\n",
    "    try:\n",
    "        headings = dataset[\"cleaned_heading\"].tolist()\n",
    "\n",
    "        embds = model.encode(\n",
    "            headings,\n",
    "            convert_to_tensor = True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    \n",
    "    return embds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embds(\n",
    "    model = model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embds(embds):\n",
    "    folder = \"Dataset\"\n",
    "    file_path = os.path.join(folder, \"embeddings.pkl\")\n",
    "    try:\n",
    "        os.makedirs(folder, exist_ok = True)\n",
    "\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            pickle.dump(embds, file)\n",
    "        print(f\"Embeddings saved successfully at {file_path}\")\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    \n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved successfully at Dataset\\embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "embd_path = save_embds(\n",
    "    embds = embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Embeddings\n",
    "embeddings = pickle.load(open(embd_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_articles_from_search(query, dataset, embeddings, model, num_recommendations = 5):\n",
    "    # Preprocess Query\n",
    "    query = preprocess_text(query)\n",
    "\n",
    "    # Encode the query\n",
    "    query_embds = model.encode([query], convert_to_tensor = True)\n",
    "\n",
    "    # Calculating Similarity\n",
    "    similarities = cosine_similarity(query_embds.reshape(1, -1), embeddings)\n",
    "    similarities = similarities.flatten()\n",
    "\n",
    "    # Get the indices of the top recommendations\n",
    "    top_indices = similarities.argsort()[-num_recommendations:][::-1]\n",
    "\n",
    "    # Select the recommended articles\n",
    "    recommended_articles = dataset.iloc[top_indices][[\"Heading\", \"NewsType\", \"Article\", \"Date\"]]\n",
    "\n",
    "    return recommended_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Heading</th>\n",
       "      <th>NewsType</th>\n",
       "      <th>Article</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>most asian markets up tokyo at 15 year hig</td>\n",
       "      <td>business</td>\n",
       "      <td>Hong Kong: Japanese shares hit a 15-year high ...</td>\n",
       "      <td>4/22/2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>asian markets mostly recover from hefty sell off</td>\n",
       "      <td>business</td>\n",
       "      <td>Hong Kong: Most Asia shares rose Wednesday as ...</td>\n",
       "      <td>7/29/2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2610</th>\n",
       "      <td>Dollar down Trump takes over Asia markets u</td>\n",
       "      <td>business</td>\n",
       "      <td>strong&gt;HONG KONG: The dollar retreated against...</td>\n",
       "      <td>1/23/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>asia markets mostly down shanghai up a 7th str...</td>\n",
       "      <td>business</td>\n",
       "      <td>Hong Kong: Asian markets mostly fell Friday fo...</td>\n",
       "      <td>7/24/2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>Asia stocks edge up to four month high after W...</td>\n",
       "      <td>business</td>\n",
       "      <td>strong&gt;TOKYO: Asian shares edged up to a four-...</td>\n",
       "      <td>3/31/2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Heading  NewsType  \\\n",
       "119          most asian markets up tokyo at 15 year hig  business   \n",
       "226    asian markets mostly recover from hefty sell off  business   \n",
       "2610        Dollar down Trump takes over Asia markets u  business   \n",
       "220   asia markets mostly down shanghai up a 7th str...  business   \n",
       "546   Asia stocks edge up to four month high after W...  business   \n",
       "\n",
       "                                                Article       Date  \n",
       "119   Hong Kong: Japanese shares hit a 15-year high ...  4/22/2015  \n",
       "226   Hong Kong: Most Asia shares rose Wednesday as ...  7/29/2015  \n",
       "2610  strong>HONG KONG: The dollar retreated against...  1/23/2017  \n",
       "220   Hong Kong: Asian markets mostly fell Friday fo...  7/24/2015  \n",
       "546   strong>TOKYO: Asian shares edged up to a four-...  3/31/2016  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Asian markets upswing\"\n",
    "\n",
    "recommended_articles = recommend_articles_from_search(query, dataset, embeddings, model)\n",
    "\n",
    "recommended_articles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
